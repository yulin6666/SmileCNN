{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this step we're going to use Keras. This will also start up your GPU if you're using one, which can take up to **10 seconds**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Reshape\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the data, and convert the labels to categories. So `0` becomes `[1, 0]` and `1` becomes `[0, 1]`. This makes it easy to add more classes later (like \"angry\", \"sad\", etc.) and interpret the predictions as probabilities. We do this after loading the file instead of before saving to avoid having a big labels file on disk.\n",
    "\n",
    "Then we shuffle all the examples to make sure we don't hold out only one class for validation. And finally we count up how many instances there are of each class to make ensure that we put more emphasis on the rarer ones during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 0.0 1.0 (13165, 32, 32, 1)\n",
      "float32 0.0 1.0 (13165, 2)\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "X = np.load('X.npy')\n",
    "y = np.load('y.npy')\n",
    "\n",
    "# convert classes to vector\n",
    "nb_classes = 2\n",
    "y = np_utils.to_categorical(y, nb_classes).astype(np.float32)\n",
    "\n",
    "# shuffle all the data\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# prepare weighting for classes since they're unbalanced\n",
    "class_totals = y.sum(axis=0)\n",
    "class_weight = class_totals.max() / class_totals\n",
    "\n",
    "print X.dtype, X.min(), X.max(), X.shape\n",
    "print y.dtype, y.min(), y.max(), y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up our network. It is based on the Keras `mnist_cnn.py` example, following in the footsteps of VGG net by using small 3x3 convolutions with max pooling, and a final stage of multiple dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 30, 30, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 812,770\n",
      "Trainable params: 812,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nb_filters = 32\n",
    "nb_pool = 2\n",
    "nb_conv = 3\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(nb_filters, (nb_conv, nb_conv), activation='relu', input_shape=X.shape[1:]))\n",
    "model.add(Conv2D(nb_filters, (nb_conv, nb_conv), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data and model is ready, we can train the model on the data for a few epochs, holding out 10% of the data for validating the accuracy. This should take about **30 seconds**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/py2version/lib/python2.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/py2version/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/py2version/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 11848 samples, validate on 1317 samples\n",
      "Epoch 1/25\n",
      "11848/11848 [==============================] - 20s 2ms/step - loss: 0.4894 - acc: 0.7627 - val_loss: 0.3441 - val_acc: 0.8565\n",
      "Epoch 2/25\n",
      "11848/11848 [==============================] - 18s 1ms/step - loss: 0.3404 - acc: 0.8569 - val_loss: 0.3136 - val_acc: 0.8664\n",
      "Epoch 3/25\n",
      "11848/11848 [==============================] - 18s 1ms/step - loss: 0.3074 - acc: 0.8766 - val_loss: 0.2859 - val_acc: 0.8762\n",
      "Epoch 4/25\n",
      "11848/11848 [==============================] - 21s 2ms/step - loss: 0.2851 - acc: 0.8905 - val_loss: 0.2637 - val_acc: 0.8914\n",
      "Epoch 5/25\n",
      "11848/11848 [==============================] - 18s 2ms/step - loss: 0.2656 - acc: 0.8989 - val_loss: 0.2886 - val_acc: 0.8770\n",
      "Epoch 6/25\n",
      "11848/11848 [==============================] - 18s 1ms/step - loss: 0.2546 - acc: 0.9029 - val_loss: 0.2602 - val_acc: 0.8861\n",
      "Epoch 7/25\n",
      "11848/11848 [==============================] - 20s 2ms/step - loss: 0.2390 - acc: 0.9085 - val_loss: 0.2376 - val_acc: 0.9036\n",
      "Epoch 8/25\n",
      "11848/11848 [==============================] - 18s 2ms/step - loss: 0.2297 - acc: 0.9114 - val_loss: 0.2360 - val_acc: 0.9021\n",
      "Epoch 9/25\n",
      "11848/11848 [==============================] - 20s 2ms/step - loss: 0.2243 - acc: 0.9153 - val_loss: 0.2325 - val_acc: 0.9051\n",
      "Epoch 10/25\n",
      "11848/11848 [==============================] - 18s 2ms/step - loss: 0.2166 - acc: 0.9177 - val_loss: 0.2314 - val_acc: 0.9112\n",
      "Epoch 11/25\n",
      "11848/11848 [==============================] - 17s 1ms/step - loss: 0.2092 - acc: 0.9197 - val_loss: 0.2246 - val_acc: 0.9104\n",
      "Epoch 12/25\n",
      "11848/11848 [==============================] - 18s 2ms/step - loss: 0.2038 - acc: 0.9231 - val_loss: 0.2201 - val_acc: 0.9104\n",
      "Epoch 13/25\n",
      "11848/11848 [==============================] - 18s 2ms/step - loss: 0.1975 - acc: 0.9261 - val_loss: 0.2159 - val_acc: 0.9119\n",
      "Epoch 14/25\n",
      "11848/11848 [==============================] - 26s 2ms/step - loss: 0.1894 - acc: 0.9306 - val_loss: 0.2210 - val_acc: 0.9112\n",
      "Epoch 15/25\n",
      "11848/11848 [==============================] - 22s 2ms/step - loss: 0.1851 - acc: 0.9318 - val_loss: 0.2201 - val_acc: 0.9119\n",
      "Epoch 16/25\n",
      "11848/11848 [==============================] - 21s 2ms/step - loss: 0.1794 - acc: 0.9329 - val_loss: 0.2210 - val_acc: 0.9150\n",
      "Epoch 17/25\n",
      "11848/11848 [==============================] - 21s 2ms/step - loss: 0.1736 - acc: 0.9327 - val_loss: 0.2198 - val_acc: 0.9134\n",
      "Epoch 18/25\n",
      "11848/11848 [==============================] - 20s 2ms/step - loss: 0.1660 - acc: 0.9377 - val_loss: 0.2177 - val_acc: 0.9112\n",
      "Epoch 19/25\n",
      "11848/11848 [==============================] - 23s 2ms/step - loss: 0.1598 - acc: 0.9391 - val_loss: 0.2246 - val_acc: 0.9142\n",
      "Epoch 20/25\n",
      "11848/11848 [==============================] - 22s 2ms/step - loss: 0.1510 - acc: 0.9433 - val_loss: 0.2208 - val_acc: 0.9150\n",
      "Epoch 21/25\n",
      "11848/11848 [==============================] - 21s 2ms/step - loss: 0.1532 - acc: 0.9423 - val_loss: 0.2271 - val_acc: 0.9180\n",
      "Epoch 22/25\n",
      "11848/11848 [==============================] - 17s 1ms/step - loss: 0.1456 - acc: 0.9450 - val_loss: 0.2268 - val_acc: 0.9150\n",
      "Epoch 23/25\n",
      "11848/11848 [==============================] - 18s 2ms/step - loss: 0.1373 - acc: 0.9489 - val_loss: 0.2266 - val_acc: 0.9119\n",
      "Epoch 24/25\n",
      "11848/11848 [==============================] - 18s 2ms/step - loss: 0.1385 - acc: 0.9465 - val_loss: 0.2259 - val_acc: 0.9195\n",
      "Epoch 25/25\n",
      "11848/11848 [==============================] - 18s 1ms/step - loss: 0.1277 - acc: 0.9550 - val_loss: 0.2324 - val_acc: 0.9180\n"
     ]
    }
   ],
   "source": [
    "validation_split = 0.10\n",
    "history = model.fit(X, y, batch_size=128, class_weight=class_weight, epochs=25, verbose=1, validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That got us to 90% validation accuracy, following the training accuracy pretty closely. To get it down more we might try tweaking the hyperparameters (number of filters, size of dense layers, etc.) or lowering the learning rate after a few epochs. But for now we will just save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/envs/py2version/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/py2version/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/py2version/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/py2version/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/py2version/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/py2version/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "open('model.json', 'w').write(model.to_json())\n",
    "model.save_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visually check the accuracy and loss, we can plot them to verify that there aren't any unexpected kinks or noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus (optional): let's check the area under the receiver operating characteristic curve (ROC AUC) so we can compare to other work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9631775787237973\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "n_validation = int(len(X) * validation_split)\n",
    "y_predicted = model.predict(X[-n_validation:])\n",
    "print roc_auc_score(y[-n_validation:], y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:TensorFlow version 1.15.0 detected. Last version known to be fully compatible is 1.14.0 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : conv2d_3_input, <keras.engine.input_layer.InputLayer object at 0x1a482c98d0>\n",
      "1 : conv2d_3, <keras.layers.convolutional.Conv2D object at 0x1a482c9750>\n",
      "2 : conv2d_3__activation__, <keras.layers.core.Activation object at 0x1a5d3bc550>\n",
      "3 : conv2d_4, <keras.layers.convolutional.Conv2D object at 0x1a482c97d0>\n",
      "4 : conv2d_4__activation__, <keras.layers.core.Activation object at 0x1a5d3bc710>\n",
      "5 : max_pooling2d_2, <keras.layers.pooling.MaxPooling2D object at 0x1a42b9dc50>\n",
      "6 : flatten_2, <keras.layers.core.Flatten object at 0x1a48a4d350>\n",
      "7 : dense_3, <keras.layers.core.Dense object at 0x1a48a4d190>\n",
      "8 : dense_3__activation__, <keras.layers.core.Activation object at 0x1a5d3bc7d0>\n",
      "9 : dense_4, <keras.layers.core.Dense object at 0x1a48a54cd0>\n",
      "10 : dense_4__activation__, <keras.layers.core.Activation object at 0x1a5d3bc890>\n"
     ]
    }
   ],
   "source": [
    "#转成corml模型\n",
    "from keras.models import load_model\n",
    "import coremltools\n",
    "\n",
    "coreml_model = coremltools.converters.keras.convert(model,input_names='data', image_input_names='data')\n",
    "coreml_model.save('smile.mlmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
